# 모델 서빙

## Deployment

`Deployment` 리소스는 이름처럼 배포에 특화된 기능이 있습니다. 변경점이 발생하여 새로운 `Pod`를 생성때 기존 `Pod` history를 전부 기억하고 있어 필요할 때마다 손쉽게 rollback을 할 수 있게 해줍니다. `Deployment`는 다음과 같은 기능을 가집니다. 

- `Pod` 복제
- 롤링 업데이트
- 배포 히스토리 저장
- 롤백 기능

먼저 `Deployment` 리소스를 생성해 보겠습니다. `Deployment`의 정의서는 `ReplicaSet`과 거의 유사하지만 배포 전략 (`strategy`)을 취할 수 있습니다.

```bash
cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mynginx-deploy
  labels:
    app: nginx
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%  
      maxSurge: 25%
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
EOF
```

- `replicas`: `ReplicaSet`과 마찬가지로 유지할 `Pod`의 개수를 정의합니다.
- `selector.matchLabels`: 라벨링 시스템을 이용하여 배포를 수행할 `Pod`를 선택합니다.
- `strategy.type`: 배포전략 종류를 선택합니다. `RollingUpdate`와 `Recreate`이 있습니다. `RollingUpdate` 타입 선택 시, 점진적으로 업데이트가 일어납니다. 서비스 중단 없이 어플리케이션을 배포할 수 있습니다. 이때 다음 설정값(`strategy.rollingUpdate`)에 따라 얼만큼 점진적으로 업데이트할지 설정할 수 있습니다. `Recreate` 타입을 선택 시, 일시적으로 전체 `Pod`가 삭제되고 새로운 `Pod`가 다시 생성됩니다. 개발 중인 `Deployment`를 제외하고는 대부분의 경우 `RollingUpdate`를 사용합니다.
- `strategy.rollingUpdate.maxUnavailable`: 최대 중단 `Pod` 허용 개수(혹은 비율)를 지정합니다. 예를 들어 총 10개 replica에 `maxUnavailable`의 비율이 25%인 경우, 약 2개(소수점 내림)의 기존 `Pod`가 `RollingUpdate` 중에 일시 중단될 수 있다는 것을 의미합니다.
- `strategy.rollingUpdate.maxSurge`: 최대 동시 `Pod` 허용 개수(혹은 비율)를 지정합니다. 예를 들어 총 10개 replica에 `maxSurge`의 비율이 25%인 경우, 약 3개(소수점 올림)의 새로운 `Pod`가 초과하여 최대 13개까지 생성될 수 있다는 것을 의미합니다. 
- `template`: 복제할 `Pod`를 정의합니다. 앞에서 살펴 본 `Pod`의 spec과 동일합니다.(`metadata`, `spec`)

`maxUnavailable`과 `maxSurge`는 `strategy.type`이 `rollingUpdate`일 경우에만 사용할 수 있습니다. 예제의 `Deployment`를 이용하여 어플리케이션을 업데이트하면 최소 8개에서 최대 13개까지의 `Pod`가 점진적으로 업데이트가 됩니다.

![](https://github.com/hongkunyoo/handson-k8s/raw/master/07-controller/07-01.png)

`Deployment` 리소스를 생성합니다. 전체 `Pod` 생성이 완료될 때까지 시간이 조금 걸릴 수도 있습니다. `--record` 옵션은 뒤에서 배포 히스토리를 확인할 때 살펴 볼 예정입니다.

```bash
kubectl apply --record -f mydeploy.yaml
# deployment.apps/mydeploy created

kubectl get deployment  # 축약 시, deploy
# NAME       READY   UP-TO-DATE   AVAILABLE   AGE
# mydeploy   10/10   10           10          10m

kubectl get pod
# NAME                   READY   STATUS       RESTARTS   AGE
# mydeploy-649xxx-bbxx   1/1     Running      0          9s
# mydeploy-649xxx-dtxx   1/1     Running      0          2m9s
# ...
```

`Deployment` 리소스를 이용하여 어플리케이션을 생성하였습니다. `mydeploy`라는 `Deployment` 리소스가 `Pod` 복제본들을 생성하였습니다.

이제 새로운 버전의 이미지로 업데이트 시, 롤링 업데이트가 되는지 확인해 보겠습니다. `kubectl set image` 라는 명령을 이용하여 nginx의 버전을 기존 `1.7.9`에서 `1.9.1`로 업그레이드합니다.

```bash
# 이미지 주소 변경
kubectl set image deployment <NAME> <CONTAINER_NAME>=<IMAGE>
```

-LINE_SPLITTER-

```bash
# 기존 nginx 버전 1.7.9에서 1.9.1로 업데이트
kubectl set image deployment mydeploy nginx=nginx:1.9.1 --record
# deployment.apps/mydeploy image updated

# 업데이트 진행 상황 확인합니다.
kubectl get pod
# NAME                   READY   STATUS             RESTARTS   AGE
# mydeploy-649xxx-bbxx   1/1     ContainerCreating  0          9s
# mydeploy-649xxx-dtxx   1/1     Running            0          2m9s
# ...

# 배포 상태확인
kubectl rollout status deployment mydeploy
# Waiting for deployment "mydeploy" rollout to finish: 
# 7 out of 10 new replicas have been updated...
# Waiting for deployment "mydeploy" rollout to finish: 
# 7 out of 10 new replicas have been updated...
# Waiting for deployment "mydeploy" rollout to finish: 
# 7 out of 10 new replicas have been updated...
# Waiting for deployment "mydeploy" rollout to finish: 
# 8 out of 10 new replicas have been updated...
# ...
# deployment "mydeploy" successfully rolled out

# 특정 Pod의 이미지 tag 정보를 확인합니다.
kubectl get pod mydeploy-xxx-xxx -o yaml | grep "image: nginx"
#   - image: nginx:1.9.1
```

한번에 모든 `Pod`가 업데이트되는 것이 아니라 `rollingUpdate` 설정값에 따라 점진적으로 새로운 `Pod`가 생성되는 것을 확인할 수 있습니다. 이렇게 `Deployment` 리소스의 롤링 업데이트 기능과 `Service` 리소스의 안정된 서비스 끝점(Stable Service Endpoint)을 통해 중단 없이 어플리케이선을 배포할 수 있습니다.

이번에는 `Deployment` 리소스의 롤백 기능에 대해서 살펴 봅시다. 서비스를 운영하다보면 어플리케이션 배포 시 의도치 않게 문제가 발생할 수 있습니다. Deployment 리소스는 이러한 상황을 염두하고 만들어졌습니다. 배포 과정에 문제 상황을 만들어 보겠습니다. 의도적으로 존재하지 않는 nginx 버전으로 이미지를 수정하고 배포 상태를 확인해 보겠습니다.

```bash
# 1.9.1 버전에서 (존재하지 않는) 1.9.21 버전으로 업데이트 (에러 발생)
kubectl set image deployment mydeploy nginx=nginx:1.9.21 --record
# deployment.apps/mydeploy image updated

# Pod의 상태확인
kubectl get pod
# NAME                  READY   STATUS            RESTARTS  AGE
# mydeploy-6498-bbk9v   1/1     Running           0         9m38s
# mydeploy-6498-dt5d7   1/1     Running           0         9m28s
# mydeploy-6498-wrpgt   1/1     Running           0         9m38s
# mydeploy-6498-sbkzz   1/1     Running           0         9m27s
# mydeploy-6498-hclwx   1/1     Running           0         9m26s
# mydeploy-6498-98hd5   1/1     Running           0         9m25s
# mydeploy-6498-5gjrg   1/1     Running           0         9m24s
# mydeploy-6498-4lz4p   1/1     Running           0         9m38s
# mydeploy-6fbf-7kzpf   0/1     ErrImagePull      0         48s
# mydeploy-6fbf-rfgbd   0/1     ErrImagePull      0         48s
# mydeploy-6fbf-v5ms5   0/1     ErrImagePull      0         48s
# mydeploy-6fbf-rccw4   0/1     ErrImagePull      0         48s
# mydeploy-6fbf-ncqd2   0/1     ImagePullBackOff  0         48s
```

잘못된 버전 설정으로 신규 `Pod`가 정상적으로 생성되지 않고 `ErrImagePull` 에러 상태에 있습니다. `maxUnavailable` 값으로 인해 최소 8개의 `Running` 중인 `Pod`를 유지하고 있고 새로 생성된 `Pod`가 정상적으로 동작하지 않기 때문에 더 이상 기존 `Pod`를 삭제하지 않고 있습니다. 반면 `maxSurge` 설정값으로 최대 `Pod` 개수가 13개를 넘지 않습니다. 잘못 배포한 것을 인지하여 배포 설정을 이전으로 롤백합니다.

```bash
# 배포 히스토리 확인
kubectl rollout history deployment <NAME>

# 이전 버전으로 롤백 명령
kubectl rollout undo deployment <NAME>
```

```bash
# 지금까지의 배포 히스토리를 확인합니다.
kubectl rollout history deployment mydeploy
# deployment.apps/mydeploy
# REVISION  CHANGE-CAUSE
# 1         kubectl apply --record=true --filename=mydeploy.yaml
# 2         kubectl set image deployment mydeploy nginx=nginx:1.9.1 
#                   --record=true
# 3         kubectl set image deployment mydeploy nginx=nginx:1.9.21 
#                   --record=true

# 잘못 설정된 1.9.21에서 --> 1.9.1로 롤백
kubectl rollout undo deployment mydeploy
# deployment.apps/mydeploy rolled back

kubectl rollout history deployment mydeploy
# deployment.apps/mydeploy
# REVISION  CHANGE-CAUSE
# 1         kubectl apply --record=true --filename=mydeploy.yaml
# 3         kubectl set image deployment mydeploy nginx=nginx:1.9.21 
#                    --record=true
# 4         kubectl set image deployment mydeploy nginx=nginx:1.9.1 
#                    --record=true

kubectl get deployment mydeploy -oyaml | grep image
# image: nginx:1.9.1
```

- 지금까지 `--record`라는 옵션을 붙인 이유는 바로 `rollout history`에서 실제 사용한 명령을 기록하기 위해서였습니다. `--record` 옵션을 사용하지 않으면 사용한 명령이 `<NONE>`으로 표시됩니다.
- `rollout undo` 명령을 통해 이전 배포 상태로 롤백할 수 있었습니다.
- 다시 `rollout history` 명령을 수행하면 직전 배포 버전인 `2`가 최종 배포 버전인 `4`로 옮겨진 것을 확인할 수 있습니다.

직접 배포 버전(revision)을 명시하여 곧바로 해당 버전으로 롤백할 수도 있습니다.

```bash
# 1.9.1 --> 1.7.9 (revision 1)로 롤백 (처음으로 롤백)
kubectl rollout undo deployment mydeploy --to-revision=1
# deployment.apps/mydeploy rolled back
```

이번에는 배포된 `Pod`의 개수를 줄여 보겠습니다. `Deployment` 리소스도 `ReplicaSet`과 마찬가지로 `scale` 명령으로 `Pod` 개수를 조절할 수 있습니다.

```bash
# 복제본 개수 조절
kubectl scale deployment --replicas <NUMBER> <NAME>
```

```bash
kubectl scale deployment mydeploy --replicas 5
# deployment.apps/mydeploy scaled

# 10개에서 5개로 줄어가는 것을 확인할 수 있습니다.
kubectl get pod
# NAME                  READY   STATUS           RESTARTS  AGE
# mydeploy-6498-bbk9v   1/1     Running          0         9m38s
# mydeploy-6498-dt5d7   1/1     Running          0         9m28s
# mydeploy-6498-wrpgt   1/1     Running          0         9m38s
# mydeploy-6498-sbkzz   1/1     Running          0         9m27s
# mydeploy-6498-98hd5   1/1     Running          0         9m27s
# mydeploy-6498-3srxd   0/1     Terminating      0         9m25s
# mydeploy-6498-5gjrg   0/1     Terminating      0         9m24s
# mydeploy-6498-4lz4p   0/1     Terminating      0         9m38s
# mydeploy-6fbf-7kzpf   0/1     Terminating      0         9m38s
# mydeploy-6fbf-d245c   0/1     Terminating      0         9m38s

# 다시 Pod의 개수를 10개로 되돌립니다.
kubectl scale deployment mydeploy --replicas=10
# deployment.apps/mydeploy scaled

# 5개가 새롭게 추가되어 다시 10개가 됩니다.
kubectl get pod
# NAME                  READY   STATUS              RESTARTS  AGE
# mydeploy-6498-bbk9v   1/1     Running             0         9m38s
# mydeploy-6498-dt5d7   1/1     Running             0         9m28s
# mydeploy-6498-wrpgt   1/1     Running             0         9m38s
# mydeploy-6498-sbkzz   1/1     Running             0         9m27s
# mydeploy-6498-98hd5   1/1     Running             0         9m25s
# mydeploy-6498-30cs2   0/1     ContainerCreating   0         5s
# mydeploy-6fbf-sdjc8   0/1     ContainerCreating   0         5s
# mydeploy-6498-w8fkx   0/1     ContainerCreating   0         5s
# mydeploy-6498-qw89f   0/1     ContainerCreating   0         5s
# mydeploy-6fbf-19glc   0/1     ContainerCreating   0         5s
```

마지막으로 `Deployment` 리소스도 `edit` 명령으로 직접 YAML 파일을 수정할 수 있습니다.

```bash
kubectl edit deploy mydeploy
# apiVersion: apps/v1
# kind: Deployment
# metadata:
# ...
# spec:
#   progressDeadlineSeconds: 600
#   replicas: 10                  # --> 3으로 수정
#   revisionHistoryLimit: 10
#   selector:
#     matchLabels:
#       run: nginx
# 
# <ESC> + :wq
```

`replicas` 개수를 3으로 수정하고 `vim` 에디터를 저장 & 종료합니다. `Pod` 개수가 줄어 든 것을 확인할 수 있습니다.

```bash
kubectl get pod
# NAME                  READY   STATUS     RESTARTS  AGE
# mydeploy-6498-bbk9v   1/1     Running    0         12m8s
# mydeploy-6498-dt5d7   1/1     Running    0         12m8s
# mydeploy-6498-wrpgt   1/1     Running    0         12m8s
```

이번 워크샵에서는 어플리케이션 배포를 책임지는 `Deployment` 리소스에 대해 살펴 봤습니다. 쿠버네티스에서 많은 경우 이 `Deployment` 리소스를 이용하여 서비스를 운영합니다. 

`Deployment` 리소스를 정리합니다.

```bash
# deployment 정리
kubectl delete deploy --all
```

## Service 리소스

`Service`에는 총 4가지 타입이 있습니다.
차례대로 살펴보도록 하겠습니다.

### ClusterIP

이번에 `Pod`의 template을 생성할때, `--expose`, `--port 80` 옵션을 추가합니다.
```bash
kubectl run mynginx --image nginx --restart Never --expose --port 80 --dry-run -o yaml > mynginx-svc.yaml
```

`mynginx-svc.yaml` 파일을 열어보면 가장 기본이 되는 `Service` 리소스가 생성된 것을 확인할 수 있습니다.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mynginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: mynginx
---
# Pod 부분 생략
...
```

`---` 지시자를 이용하여 두개의 리소스를 동일한 파일에 작성할 수 있습니다.

`Pod`와 마찬가지로 `apiVersion`, `kind`, `metadata` 등의 property가 존재합니다.
`spec` 부분에서 `Service` 고유의 명세(description)가 정의됩니다.
- `ports`: 포트 리스트를 가집니다.
- `port`: 컨테이너 바깥쪽에서 Open할 포트 번호를 정의합니다.
- `protocol`: 전송에 사용할 프로토콜을 정의합니다. (생략시 TCP)
- `targetPort`: 컨테이너 안쪽에서 사용할 포트 번호를 정의합니다.
- `selector`: 바로 이 부분이 Pod와의 연결고리를 만듭니다. `Service`는 들어오는 트래픽을 특정 `Pod`에 보내지 않고 label의 key가 `run`이고 value가 `mynginx`를 가진 모드 `Pod`에게 트래픽을 전달합니다. 쿠버네티스는 왜 이런 매커니즘을 채택하였을까요? 바로 확장성 때문인데요, 특정 `Pod`를 지정하게 되면 한개 `Pod`로 밖에 트래픽을 전달하지 못하지만 label을 통하여 트래픽을 전달하면 그 라벨을 가진 모든 `Pod`에 트래픽을 전달할 수 있기 때문에 여러 `Pod`를 두어 부하를 분산하여 서비스할 수 있는 효과를 가집니다. (마치 mini loadbalancer 처럼)

이제 생성한 `Pod`로 호출해 보겠습니다.
```bash
kubectl apply -f mynginx-svc.yaml
# service/mynginx created
# pod/mynginx created

kubectl get service  # 축약시, svc
# NAME          TYPE          CLUSTER-IP     EXTERNAL-IP    PORT(S)    AGE
# kubernetes    ClusterIP     10.43.0.1      <none>         443/TCP    5h
# mynginx       ClusterIP     10.43.230.45   <none>         80/TCP     56s
```

- `kubernetes` 서비스는 쿠버네티스 디폴트 서비스입니다. (마스터 API서버 주소)
- `mynginx`는 방금 생성한 서비스입니다.

CLUSTER-IP 선택, 예시에서는 10.43.230.45

```bash
kubectl run mycurl --image curlimages/curl --restart Never -- curl 10.43.230.45
# pod/mycurl created

kubectl logs mycurl
# ... <nginx home page> ...

# Service의 이름으로도 dns resolve가 됩니다.
kubectl run mycurl-name --image curlimages/curl --restart Never -- curl mynginx
kubectl logs mycurl-name
```

```bash
kubectl get svc mynginx -oyaml | grep type
#  type: ClusterIP
```

mynginx `Service`의 타입을 검색해보면 `ClusterIP`라고 되어 있는 것을 확인할 수 있습니다. 앞서 YAML에서는 따로 type을 지정하지 않았지만 생략시, default로 `ClusterIP`가 선택됩니다.
`ClusterIP`의 특징으로 도커 네트워크와는 다르게 호스트에서 직접 접근하지 못하고 다른 `Pod`를 통해서만 통신이 가능합니다.

```bash
# 호스트 서버에서 curl
$ curl 10.43.230.45
# error or timeout
```

---

### LoadBalancer

`NodePort`까지 다루면 도커에서 다룬 내용까지 전부 커버합니다. 쿠버네티스는 네트워크 기능을 더 추가하여 퍼블릭 클라우드 플랫폼에서 제공하는 LoadBalancer를 `Service` 리소스를 이용하여 붙일 수 있습니다.
다행히 k3s에서 퍼블릭 클라우드의 LoadBalancer를 똑같이 구현한 가상 LB를 제공해주기 때문에 똑같은 방식으로 실습해 볼 수 있습니다.
외부 LoadBalancer를 연결한다고 그리 어려운 작업이 아닙니다. 사실 type만 변경하는 것이 전부입니다.
```bash
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: mynginx-lb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: mynginx
EOF
```

```bash
kubectl get svc
# NAME         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
# kubernetes   ClusterIP      10.43.0.1      <none>           443/TCP        2d3h
# mynginx      LoadBalancer   10.43.230.45   172.31.19.148    80:30080/TCP   31m
```

기존과는 다르게 `EXTERNAL-IP`라는 컬럼에 IP가 생성된 것을 확인할 수 있습니다. (예시에서는 172.31.19.148) `LoadBalancer` 타입을 이용하면 외부IP를 할당 받아 사용할 수 있습니다.

```bash
curl 172.31.19.148
```

*k3s에서는 가상 LB를 사용하기 때문에 실질적으로는 호스트IP를 부여 받습니다만 정식 k8s 클러스터를 사용하게 되면 외부에서 직접 접근이 가능한 IP를 부여 받게 됩니다.*

```bash
# 호스트의 외부 IP가 있는 경우에 한해서 예시 (출력되는 IP는 사용자마다 다릅니다.)
curl ifconfig.co
# 13.10.52.63

curl 13.10.52.63
```

### 참고할 자료

- 쿠버네티스 네트워킹 이해하기: https://coffeewhale.com/k8s/network/2019/04/19/k8s-network-01/
- 네트워크 구조: https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model
- Borg: https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/


#### Clean up

```bash
kubectl delete pod --all
kubectl delete svc --all
```

---

## 모델서빙 코드

다음과 같이 모델을 서빙하는 스크립트가 있습니다. S3에서 모델을 읽어와서 모델을 서빙하는 flask 코드입니다.

```python
# app.py
from flask import Flask, send_file

import matplotlib.pyplot as plt
import io
import tensorflow as tf

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop

import numpy as np
from smart_open import open

app = Flask(__name__)

#################################
# 모델을 S3로부터 불러옵니다.
#################################
def get_model():
    activate = 'softmax'
    dropout = float(0)

    batch_size, num_classes, hidden = (128, 10, 512)
    loss_func = "categorical_crossentropy"
    # smart_open을 이용하여 S3에서 모델파일을 불러옵니다.
    with open('s3://mlops-2020-10/mymodel.npy', 'rb') as f:
        weights = np.load(f, allow_pickle=True)
        # build model
        model = Sequential()
        model.add(Dense(hidden, activation='relu', input_shape=(784,)))
        model.add(Dropout(dropout))
        model.add(Dense(num_classes, activation=activate))

        model.compile(loss=loss_func, optimizer=RMSprop(), metrics=['accuracy'])
        model.set_weights(weights)
    return model

model = get_model()
##########################
# 숫자를 예측합니다.
##########################
@app.route('/predict/<num>')
def predict(num):
    # 숫자에 대응되는 numpy 이미지를 불러옵니다.
    x = get_image(int(num))

    # 모델 예측하기
    with tf.get_default_graph().as_default():
        y = model.predict(x.reshape(1, 784))
    print("Predict: ", np.argmax(y))

    mem = io.BytesIO()
    plt.imsave(mem, x)
    mem.seek(0)

    return send_file(mem, mimetype="image/png")


# 숫자에 대응하는 이미지를 불러옵니다.
def get_image(num):
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    idx = [3, 2,1,18,4,15,11,0,61,7]
    return x_test[idx[num]]


if __name__ == '__main__':
    app.run(host="0.0.0.0", debug=True)
```

```bash
pip3 install matplotlib smart_open tensorflow keras

python app.py
```

웹 페이지에서 다음과 같이 실행해봅니다.

`$EC2_IP:5000/predict/<NUMBER>`

예를 들어 `$EC2_IP:5000/predict/1`라고 호출하면 다음과 같은 예측 결과가 나오게 됩니다.

![](1.png)

## :trophy: Do it more

위에서 만든 모델서빙 코드를 쿠버네티스 `Deployment`, `Service` 리소스로 만들어서 배포해 주시기 바랍니다. `Service` 타입은 `LoadBalancer`의 `80`포트로 열어주시기 바랍니다. 서비스를 배포한 이후 다음과 같이 동일하게 테스트해봅니다. 

웹 브라우저에서 `$EC2_IP/predict/1` 호출 > 이전과 동일하게 결과가 나오면 성공

`Deployment`와 `Service` 리소스를 합친 `serving.yaml` 라는 이름으로 파일을 제출해 주시기 바랍니다.
